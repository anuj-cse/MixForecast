{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./model')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.model import MixForecast\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, ConcatDataset\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import os\n",
    "\n",
    "# def standardize_series(series, mean=None, std=None, eps=1e-8):\n",
    "#     if mean is None or std is None:\n",
    "#         mean = np.mean(series)\n",
    "#         std = np.std(series)\n",
    "#     standardized_series = (series - mean) / (std + eps)\n",
    "#     return standardized_series, mean, std\n",
    "\n",
    "# def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "#     return predictions * (std + eps) + mean\n",
    "\n",
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, data, backcast_length, forecast_length, stride=1, mean=None, std=None):\n",
    "#         # Standardize the time series data with provided mean and std\n",
    "#         self.data, self.mean, self.std = standardize_series(data, mean, std)\n",
    "#         self.backcast_length = backcast_length\n",
    "#         self.forecast_length = forecast_length\n",
    "#         self.stride = stride\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         start_index = index * self.stride\n",
    "#         x = self.data[start_index : start_index + self.backcast_length]\n",
    "#         y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "#         return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_datasets(folder_path, backcast_length, forecast_length, stride):\n",
    "#     train_datasets = []\n",
    "#     val_datasets = []\n",
    "#     test_datasets = []\n",
    "    \n",
    "#     # Initialize mean and std to None, to be computed from train data\n",
    "    \n",
    "\n",
    "#     for region in os.listdir(folder_path):\n",
    "#         region_path = os.path.join(folder_path, region)\n",
    "#         for building in os.listdir(region_path):\n",
    "\n",
    "#             mean, std = None, None\n",
    "\n",
    "#             if building.endswith('.csv'):\n",
    "#                 file_path = os.path.join(region_path, building)\n",
    "#                 df = pd.read_csv(file_path)\n",
    "#                 energy_data = df['energy'].values\n",
    "                \n",
    "#                 # Split the energy data into train, val, test with 0.4, 0.1, and 0.5 ratios\n",
    "#                 train_data, temp_data = train_test_split(energy_data, test_size=0.6, shuffle=False)  # 60% temp_data\n",
    "#                 val_data, test_data = train_test_split(temp_data, test_size=0.8333, shuffle=False)  # 50% of temp_data for test\n",
    "\n",
    "#                 # Standardize only the train data and get mean, std\n",
    "#                 if mean is None or std is None:\n",
    "#                     train_data, mean, std = standardize_series(train_data)\n",
    "\n",
    "#                 # Create TimeSeriesDataset for each split using the mean and std from train data\n",
    "#                 train_dataset = TimeSeriesDataset(train_data, backcast_length, forecast_length, stride, mean, std)\n",
    "#                 val_dataset = TimeSeriesDataset(val_data, backcast_length, forecast_length, stride, mean, std)\n",
    "#                 test_dataset = TimeSeriesDataset(test_data, backcast_length, forecast_length, stride, mean, std)\n",
    "\n",
    "#                 # Append datasets for each split\n",
    "#                 train_datasets.append(train_dataset)\n",
    "#                 if not building.startswith('Mathura'):\n",
    "#                     val_datasets.append(val_dataset)\n",
    "#                 test_datasets.append(test_dataset)\n",
    "\n",
    "#     # Combine all datasets for each split\n",
    "#     combined_train_dataset = ConcatDataset(train_datasets)\n",
    "#     combined_val_dataset = ConcatDataset(val_datasets)\n",
    "#     combined_test_dataset = ConcatDataset(test_datasets)\n",
    "\n",
    "#     return combined_train_dataset, combined_val_dataset, combined_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std+eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, backcast_length, forecast_length, stride=1):\n",
    "        # Standardize the time series data\n",
    "        self.data, self.mean, self.std = standardize_series(data)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.stride\n",
    "        x = self.data[start_index : start_index + self.backcast_length]\n",
    "        y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(folder_path, backcast_length, forecast_length, stride):\n",
    "    train_datasets = []\n",
    "    val_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for region in os.listdir(folder_path):\n",
    "        region_path = os.path.join(folder_path, region)\n",
    "        for building in os.listdir(region_path):\n",
    "\n",
    "            if building.endswith('.csv'):\n",
    "                file_path = os.path.join(region_path, building)\n",
    "                df = pd.read_csv(file_path)\n",
    "                energy_data = df['energy'].values\n",
    "                \n",
    "                # Split the energy data into train, val, test with 0.4, 0.1, and 0.5 ratios\n",
    "                train_data, temp_data = train_test_split(energy_data, test_size=0.6, shuffle=False)  # 60% temp_data\n",
    "                val_data, test_data = train_test_split(temp_data, test_size=0.8333, shuffle=False)  # 50% of temp_data for test\n",
    "\n",
    "                # Create TimeSeriesDataset for each split\n",
    "                train_dataset = TimeSeriesDataset(train_data, backcast_length, forecast_length, stride)\n",
    "                val_dataset = TimeSeriesDataset(val_data, backcast_length, forecast_length, stride)\n",
    "                test_dataset = TimeSeriesDataset(test_data, backcast_length, forecast_length, stride)\n",
    "\n",
    "                # Append datasets for each split\n",
    "                \n",
    "                train_datasets.append(train_dataset)\n",
    "                if not building.startswith('Mathura'):\n",
    "                    val_datasets.append(val_dataset)\n",
    "                test_datasets.append(test_dataset)\n",
    "\n",
    "                # train_datasets = [d for d in train_datasets if len(d) > 0]\n",
    "                # val_datasets = [d for d in val_datasets if len(d) > 0]\n",
    "                # test_datasets = [d for d in test_datasets if len(d) > 0]\n",
    "\n",
    "    print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "    # Combine all datasets for each split\n",
    "    combined_train_dataset = ConcatDataset(train_datasets)\n",
    "    combined_val_dataset = ConcatDataset(val_datasets)\n",
    "    combined_test_dataset = ConcatDataset(test_datasets)\n",
    "\n",
    "    return combined_train_dataset, combined_val_dataset, combined_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train(args, model, criterion, optimizer, device, train_loader, val_loader):\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = args[\"patience\"]\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    early_stop = False\n",
    "\n",
    "    num_epochs = args[\"num_epochs\"]\n",
    "    train_start_time = time()  # Start timer\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break  \n",
    "\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        epoch_start_time = time()  # Start epoch timer\n",
    "\n",
    "        # Progress bar for the training loop\n",
    "        with tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for x_batch, y_batch in pbar:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                backcast, forecast = model(x_batch)\n",
    "                loss = criterion(forecast, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                pbar.set_postfix(loss=loss.item(), elapsed=f\"{time() - epoch_start_time:.2f}s\")\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_true_val = []\n",
    "        y_pred_val = []\n",
    "\n",
    "        # Progress bar for the validation loop\n",
    "        with tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for x_val, y_val in pbar:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                with torch.no_grad():\n",
    "                    backcast, forecast = model(x_val)\n",
    "                    loss = criterion(forecast, y_val)\n",
    "                    val_losses.append(loss.item())\n",
    "                    \n",
    "                    # Collect true and predicted values for RMSE calculation\n",
    "                    y_true_val.extend(y_val.cpu().numpy())\n",
    "                    y_pred_val.extend(forecast.cpu().numpy())\n",
    "\n",
    "        # Calculate average validation loss and RMSE\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        print(len(y_true_val), len(y_pred_val))\n",
    "        print(y_true_val[0].shape, y_pred_val[0].shape)\n",
    "        # rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            # Save the best model parameters\n",
    "            os.makedirs(args[\"finetuned_model_save_path\"], exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{args[\"finetuned_model_save_path\"]}/best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                early_stop = True\n",
    "\n",
    "\n",
    "    total_training_time = time() - train_start_time\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732082/1066891437.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'{args[\"pretrained_model_path\"]}/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 29 174\n",
      "Model's parameter count is: 116238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174863 174863\n",
      "(1, 24) (1, 24)\n",
      "Epoch 1/10, Train Loss: 0.0931, Val Loss: 0.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174863 174863\n",
      "(1, 24) (1, 24)\n",
      "Epoch 2/10, Train Loss: 0.0925, Val Loss: 0.2231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174863 174863\n",
      "(1, 24) (1, 24)\n",
      "Epoch 3/10, Train Loss: 0.0921, Val Loss: 0.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174863 174863\n",
      "(1, 24) (1, 24)\n",
      "Epoch 4/10, Train Loss: 0.0917, Val Loss: 0.2255\n",
      "Early stopping at epoch 5\n",
      "Total Training Time: 126.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Time Series Forecasting')\n",
    "    # parser.add_argument('--config-file', type=str, default='./configs/model_base.json', help='Input config file path', required=True)\n",
    "    # file_path_arg = parser.parse_args()\n",
    "    # config_file = file_path_arg.config_file\n",
    "    config_file = './configs/model_base_finetune.json'\n",
    "    with open(config_file, 'r') as f:\n",
    "        args = json.load(f)\n",
    "\n",
    "\n",
    "    dataset_path = args[\"dataset_path\"]\n",
    " \n",
    "\n",
    "    # Load datasets\n",
    "    train_datasets, val_datasets, _ = load_datasets(dataset_path, args[\"seq_len\"], args[\"pred_len\"], args[\"stride\"])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_datasets, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_datasets, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # check device \n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    num_patches = args[\"seq_len\"] // args[\"patch_size\"]\n",
    "\n",
    "    # Define MixForecast model\n",
    "    model = MixForecast(\n",
    "        device=device,\n",
    "        forecast_length=args[\"pred_len\"],\n",
    "        backcast_length=args[\"seq_len\"],\n",
    "        patch_size = args[\"patch_size\"], \n",
    "        num_patches = num_patches,\n",
    "        num_features = args[\"num_features\"],\n",
    "        hidden_dim=args[\"hidden_dim\"],\n",
    "        nb_blocks_per_stack=args[\"num_blocks_per_stack\"],\n",
    "        stack_layers = args[\"stack_layers\"],\n",
    "        factor = args[\"factor\"],\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{args[\"pretrained_model_path\"]}/best_model.pth'))\n",
    "    # model.train()\n",
    "\n",
    "\n",
    "    # model's parameters\n",
    "    param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Model's parameter count is:\", param)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    if args[\"loss\"] == \"huber\":\n",
    "        criterion = torch.nn.HuberLoss(reduction=\"mean\", delta=1.0)\n",
    "    else:\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # training the model and save best parameters\n",
    "    train(args=args, model=model, criterion=criterion, optimizer=optimizer, device=device, train_loader=train_loader, val_loader=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "from time import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./models')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from models.model import MixForecast\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# metrics used for evaluation\n",
    "def cal_cvrmse(pred, true, eps=1e-8):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.power(np.square(pred - true).sum() / pred.shape[0], 0.5) / (true.sum() / pred.shape[0] + eps)\n",
    "\n",
    "def cal_mae(pred, true):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "def cal_nrmse(pred, true, eps=1e-8):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    M = len(true) // 24\n",
    "    y_bar = np.mean(true)\n",
    "    NRMSE = 100 * (1/ (y_bar+eps)) * np.sqrt((1 / (24 * M)) * np.sum((true - pred) ** 2))\n",
    "    return NRMSE\n",
    "\n",
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std+eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, backcast_length, forecast_length, stride=1):\n",
    "        # Standardize the time series data\n",
    "        self.data, self.mean, self.std = standardize_series(data)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.stride\n",
    "        x = self.data[start_index : start_index + self.backcast_length]\n",
    "        y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(args, model, criterion, device):\n",
    "\n",
    "    folder_path = args[\"dataset_path\"]\n",
    "    result_path = args[\"result_path\"]\n",
    "    \n",
    "    median_res = []  \n",
    "    for region in os.listdir(folder_path):\n",
    "\n",
    "        region_path = os.path.join(folder_path, region)\n",
    "\n",
    "        results_path = os.path.join(result_path, region)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for building in os.listdir(region_path):\n",
    "\n",
    "            building_id = building.rsplit(\".csv\",1)[0]\n",
    "\n",
    "            if building.endswith('.csv'):\n",
    "                file_path = os.path.join(region_path, building)\n",
    "                df = pd.read_csv(file_path)\n",
    "                energy_data = df['energy'].values\n",
    "\n",
    "                train_data, temp_data = train_test_split(energy_data, test_size=0.6, shuffle=False)  # 60% temp_data\n",
    "                val_data, test_data = train_test_split(temp_data, test_size=0.8333, shuffle=False)  # 50% of temp_data for test\n",
    "\n",
    "                test_dataset = TimeSeriesDataset(test_data, args[\"seq_len\"], args[\"pred_len\"], args[\"stride\"])\n",
    "                \n",
    "                # test phase\n",
    "                model.eval()\n",
    "                val_losses = []\n",
    "                y_true_test = []\n",
    "                y_pred_test = []\n",
    "\n",
    "                # test loop\n",
    "                for x_test, y_test in tqdm(DataLoader(test_dataset, batch_size=1), desc=f\"Testing {building_id}\", leave=False):\n",
    "                    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        backcast, forecast = model(x_test)\n",
    "                        loss = criterion(forecast, y_test)\n",
    "                        val_losses.append(loss.item())\n",
    "                        \n",
    "                        # Collect true and predicted values for RMSE calculation   \n",
    "                        y_test = y_test.squeeze(1)\n",
    "                        forecast = forecast.squeeze(1)\n",
    "                        y_true_test.extend(y_test.cpu().numpy())\n",
    "                        y_pred_test.extend(forecast.cpu().numpy())\n",
    "                        \n",
    "                # Calculate average validation loss and RMSE\n",
    "                y_true_combine = np.concatenate(y_true_test, axis=0)\n",
    "                y_pred_combine = np.concatenate(y_pred_test, axis=0)\n",
    "                avg_test_loss = np.mean(val_losses)\n",
    "                \n",
    "                y_pred_combine_unscaled = unscale_predictions(y_pred_combine, test_dataset.mean, test_dataset.std)\n",
    "                y_true_combine_unscaled = unscale_predictions(y_true_combine, test_dataset.mean, test_dataset.std)\n",
    "                \n",
    "                # Calculate CVRMSE, NRMSE, MAE on unscaled data\n",
    "                cvrmse = cal_cvrmse(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                nrmse = cal_nrmse(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                mae = cal_mae(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "\n",
    "                res.append([building_id, cvrmse, nrmse, mae, avg_test_loss])\n",
    "\n",
    "        columns = ['building_ID', 'CVRMSE', 'NRMSE', 'MAE', 'Avg_Test_Loss']\n",
    "        df = pd.DataFrame(res, columns=columns)\n",
    "        df.to_csv(\"{}/{}.csv\".format(results_path, 'result'), index=False)\n",
    "\n",
    "        med_nrmse = df['NRMSE'].median()\n",
    "        median_res.append([region, med_nrmse])\n",
    "\n",
    "    med_columns = ['Dataset','NRMSE']\n",
    "    median_df = pd.DataFrame(median_res, columns=med_columns)\n",
    "    median_df.to_csv(f\"{result_path}/median_buildings_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732082/3972630835.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'{args[\"finetuned_model_save_path\"]}/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's parameter count is: 116238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time taken by model is 1386.8135697841644 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Time Series Forecasting')\n",
    "    # parser.add_argument('--config-file', type=str, default='./configs/model_base.json', help='Input config file path', required=True)\n",
    "    # file_path_arg = parser.parse_args()\n",
    "    # config_file = file_path_arg.config_file\n",
    "    config_file = './configs/model_base_finetune.json'\n",
    "    with open(config_file, 'r') as f:\n",
    "        args = json.load(f)\n",
    "\n",
    "\n",
    "    # check device \n",
    "    device = args[\"device\"]\n",
    "    # device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    num_patches = args[\"seq_len\"] // args[\"patch_size\"]\n",
    "    \n",
    "    # Define MixForecast model\n",
    "    model = MixForecast(\n",
    "        device=device,\n",
    "        forecast_length=args[\"pred_len\"],\n",
    "        backcast_length=args[\"seq_len\"],\n",
    "        patch_size = args[\"patch_size\"], \n",
    "        num_patches = num_patches,\n",
    "        num_features = args[\"num_features\"],\n",
    "        hidden_dim=args[\"hidden_dim\"],\n",
    "        nb_blocks_per_stack=args[\"num_blocks_per_stack\"],\n",
    "        stack_layers = args[\"stack_layers\"],\n",
    "        factor = args[\"factor\"],\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{args[\"finetuned_model_save_path\"]}/best_model.pth'))\n",
    "\n",
    "    # model's parameters\n",
    "    param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Model's parameter count is:\", param)\n",
    "\n",
    "    # Define loss\n",
    "    if args[\"loss\"] == \"huber\":\n",
    "        criterion = torch.nn.HuberLoss(reduction=\"mean\", delta=1.0)\n",
    "    else:\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    # testing the model\n",
    "    test(args=args, model=model, criterion=criterion, device=device)\n",
    "\n",
    "\n",
    "    end_time = time() - start_time\n",
    "\n",
    "    print(f\"inference time taken by model is {end_time} sec\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBMTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
